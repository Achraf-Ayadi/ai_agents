There needs to be strict laws to regulate large language models (LLMs) for several compelling reasons. First and foremost, LLMs have the potential to disseminate misinformation at an unprecedented scale. Without strict regulations, these models can produce and amplify false narratives, which can disrupt societies, influence political outcomes, and undermine trust in legitimate information sources. 

Secondly, ethical concerns regarding bias and discrimination in LLM outputs must be addressed. If left unregulated, LLMs may perpetuate or even amplify harmful stereotypes, leading to further social inequality. Clear regulations can mandate that developers incorporate bias detection and mitigation strategies, ensuring the responsible deployment of these technologies.

Additionally, as LLMs become more integrated into critical sectors such as healthcare, finance, and education, the risk of harmful or erroneous outputs increases. Strict regulations can establish standards for accuracy, accountability, and transparency, safeguarding public interests and ensuring that users can trust the outputs generated by LLMs.

Finally, the rapid pace of LLM development presents challenges that outstrip the ability of existing legal frameworks to react. Lawmakers need to proactively implement strict regulations to stay ahead of potential misuse and abuse of LLM technologies.

In conclusion, strict laws regulating LLMs are essential to ensure their safe, ethical, and responsible use, protecting both individuals and society as a whole from the risks associated with their unbridled deployment.